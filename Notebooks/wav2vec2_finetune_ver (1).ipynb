{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d818058-a076-4917-bc1f-ef3c0218180a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e248cc4b-0951-4e34-834f-2136f7e04beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.7.0-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "Collecting jiwer\n",
      "  Using cached jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (2025.3.0)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: torch==2.7.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torchaudio) (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torch==2.7.0->torchaudio) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torch==2.7.0->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torch==2.7.0->torchaudio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torch==2.7.0->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.11.18-cp311-cp311-win_amd64.whl (443 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.0->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from jinja2->torch==2.7.0->torchaudio) (3.0.2)\n",
      "Installing collected packages: multiprocess, aiosignal, pandas, jiwer, huggingface-hub, aiohttp, torchaudio, tokenizers, transformers, datasets\n",
      "Successfully installed aiohttp-3.11.18 aiosignal-1.3.2 datasets-3.6.0 huggingface-hub-0.31.1 jiwer-3.1.0 multiprocess-0.70.16 pandas-2.2.3 tokenizers-0.21.1 torchaudio-2.7.0 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "# pip install datasets transformers torchaudio jiwer librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ec5764-37de-4b0a-9e78-f3f8acff2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c9d18f-94bf-4c1f-864b-d4df44354a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\2.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\7.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\12.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\13.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\14.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\20.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\21.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\27.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\33.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\34.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\36.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./41.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./47.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\51.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./56.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./57.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./60.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./62.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./63.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./64.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./68.wav\n",
      "Missing file: ./creolese-audio-dataset/Audio Files/finetune_eligible\\./.wav\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible\"\n",
    "transcription_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible/transcripts.json\"\n",
    "\n",
    "# Load transcripts JSON\n",
    "with open(transcription_path, 'r') as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create a list of dicts pairing audio files and transcripts\n",
    "data = []\n",
    "for item in transcripts:\n",
    "    audio_file = os.path.join(audio_path, item['audio'])\n",
    "    if os.path.exists(audio_file):\n",
    "        print(f\"Found file: {audio_file}\")\n",
    "        data.append({'audio': audio_file, 'text': item['text']})\n",
    "    else:\n",
    "        print(f\"Missing file: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9939099-8602-4e98-91d4-cb564e3dd9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 21\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Cast the audio column to automatically load audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd89aab3-4c09-4326-824e-26872d9181f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ed8551-ccd1-4636-a0d2-e4633fb78332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Load processor (tokenizer + feature extractor)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb86bb6e-7d11-4c79-b593-4c18c3531777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777eb12681e74b84bd614935c8f1b9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch, processor):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Get input values from audio\n",
    "    input_values = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values[0]\n",
    "\n",
    "    # Get labels from text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "\n",
    "    # Return proper format for CTC\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": batch[\"labels\"]\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, num_proc=4, fn_kwargs={\"processor\": processor})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f815e459-77f2-4970-bfa1-dff9350cb6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-960h\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59761ce1-eda6-4f58-af13-90e2c1a7f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed58c0-4cc1-4663-a45c-2464dbee47f0",
   "metadata": {},
   "source": [
    "# This is a custom attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4336151-5eca-4e83-9d28-c1338f9a2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "@dataclass\n",
    "class SimpleCTCDataCollator:\n",
    "        processor: Any\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            # Get the input_values from each feature\n",
    "            input_values = [feature[\"input_values\"].squeeze(0) if isinstance(feature[\"input_values\"], torch.Tensor) else torch.tensor(feature[\"input_values\"]) for feature in features]\n",
    "\n",
    "            # Determine max length for padding\n",
    "            max_length = max(len(x) for x in input_values)\n",
    "\n",
    "            # Pad the input_values manually\n",
    "            padded_input_values = []\n",
    "            attention_mask = []\n",
    "\n",
    "            for val in input_values:\n",
    "                # Create attention mask (1 for real values, 0 for padding)\n",
    "                length = len(val)\n",
    "                mask = torch.ones(length)\n",
    "                if length < max_length:\n",
    "                    pad_length = max_length - length\n",
    "                    # Pad the input values\n",
    "                    val = torch.nn.functional.pad(val, (0, pad_length), value=0.0)\n",
    "                    # Extend the attention mask with zeros for padding\n",
    "                    mask = torch.nn.functional.pad(mask, (0, pad_length), value=0.0)\n",
    "\n",
    "                padded_input_values.append(val)\n",
    "                attention_mask.append(mask)\n",
    "\n",
    "            # Stack the padded inputs and attention masks\n",
    "            batch = {\n",
    "                \"input_values\": torch.stack(padded_input_values),\n",
    "                \"attention_mask\": torch.stack(attention_mask)\n",
    "            }\n",
    "\n",
    "            # Get labels\n",
    "            if \"labels\" in features[0]:\n",
    "                labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "                # Pad labels manually with -100 (ignore index for CTC loss)\n",
    "                padded_labels = []\n",
    "                max_label_length = max(len(l) for l in labels)\n",
    "\n",
    "                for label in labels:\n",
    "                    if isinstance(label, torch.Tensor):\n",
    "                        label = label.tolist()\n",
    "\n",
    "                    if len(label) < max_label_length:\n",
    "                        # Pad with -100\n",
    "                        label = label + [-100] * (max_label_length - len(label))\n",
    "\n",
    "                    padded_labels.append(torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "                batch[\"labels\"] = torch.stack(padded_labels)\n",
    "\n",
    "            return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1470fb65-bb60-42e8-b7ac-f98f4ed742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = SimpleCTCDataCollator(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951c6c2-acf9-4313-8d08-5051688c3030",
   "metadata": {},
   "source": [
    "# This is the version that doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a752fff6-9ae0-4875-b307-299b8c477917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=processor.tokenizer, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee78054-7d3d-4366-ac84-dde99b44e2b6",
   "metadata": {},
   "source": [
    "# Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1305187-dffa-4325-b3d2-f6794064da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (2.7.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from transformers[torch]) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests->transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from requests->transformers[torch]) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\krist\\downloads\\fyp_venv\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "815a0e11-8ffc-4046-9684-81abdda296d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-creolese-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=25,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # True if on GPU with mixed precision\n",
    "    gradient_accumulation_steps=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89cc2afa-4002-4fba-8bb0-36f7a6af925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    mer = jiwer.mer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "    return {\"wer\": wer, \"mer\": mer, \"cer\": cer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2904d24e-38f8-47e7-850d-c2b2ea023920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 1/21\n",
      "Processing example 2/21\n",
      "Processing example 3/21\n",
      "Processing example 4/21\n",
      "Processing example 5/21\n",
      "Processing example 6/21\n",
      "Processing example 7/21\n",
      "Processing example 8/21\n",
      "Processing example 9/21\n",
      "Processing example 10/21\n",
      "Processing example 11/21\n",
      "Processing example 12/21\n",
      "Processing example 13/21\n",
      "Processing example 14/21\n",
      "Processing example 15/21\n",
      "Processing example 16/21\n",
      "Processing example 17/21\n",
      "Processing example 18/21\n",
      "Processing example 19/21\n",
      "Processing example 20/21\n",
      "Processing example 21/21\n"
     ]
    }
   ],
   "source": [
    "# To avoid maxing out ram\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create a directory to store processed features\n",
    "os.makedirs(\"processed_features\", exist_ok=True)\n",
    "\n",
    "# Process each example once and save to disk\n",
    "for idx, example in enumerate(dataset):\n",
    "    print(f\"Processing example {idx+1}/{len(dataset)}\")\n",
    "\n",
    "    # Get audio\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    # Extract features\n",
    "    input_values = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values[0]\n",
    "\n",
    "    # Get labels\n",
    "    labels = processor.tokenizer(example[\"text\"]).input_ids\n",
    "\n",
    "    # Save to disk\n",
    "    torch.save({\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": labels\n",
    "    }, f\"processed_features/example_{idx}.pt\")\n",
    "\n",
    "# Create a custom dataset that loads from disk\n",
    "class AudioFeatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, feature_dir, num_examples):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.num_examples = num_examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load features from disk\n",
    "        features = torch.load(f\"{self.feature_dir}/example_{idx}.pt\")\n",
    "        return features\n",
    "\n",
    "# Use the disk-based dataset\n",
    "train_dataset = AudioFeatureDataset(\"processed_features\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987bf2f4-e10b-4ee4-805f-4a41498411d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\AppData\\Local\\Temp\\ipykernel_34396\\477740428.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=SimpleCTCDataCollator(processor=processor),\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=processed_dataset,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4016ccd-3b4f-4c57-a03d-622e14cc6cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krist\\Downloads\\fyp_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/125 : < :, Epoch 0.19/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b60d5d-1a67-4062-9c29-ffa67a3d6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n",
    "processor.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af321de-0f77-410a-ba8c-87a69999b867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
