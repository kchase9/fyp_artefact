{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d818058-a076-4917-bc1f-ef3c0218180a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248cc4b-0951-4e34-834f-2136f7e04beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets transformers torchaudio jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1ec5764-37de-4b0a-9e78-f3f8acff2892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c9d18f-94bf-4c1f-864b-d4df44354a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ./creolese-audio-dataset/Audio Files/1.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/2.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/3.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/4.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/5.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/6.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/7.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/8.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/9.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/10.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/11.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/12.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/13.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/14.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/15.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/16.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/17.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/18.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/19.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/20.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/21.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/22.wav\n",
      "Found file: ./creolese-audio-dataset/./Audio Files/23.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/24.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/25.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/26.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/27.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/28.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/29.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/30.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/31.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/32.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/33.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/34.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/35.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/36.wav\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"./creolese-audio-dataset/\"\n",
    "transcription_path = \"./creolese-audio-dataset/transcripts.json\"\n",
    "\n",
    "# Load transcripts JSON\n",
    "with open(transcription_path, 'r') as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create a list of dicts pairing audio files and transcripts\n",
    "data = []\n",
    "for item in transcripts:\n",
    "    audio_file = os.path.join(audio_path, item['audio'])\n",
    "    if os.path.exists(audio_file):\n",
    "        print(f\"Found file: {audio_file}\")\n",
    "        data.append({'audio': audio_file, 'text': item['text']})\n",
    "    else:\n",
    "        print(f\"Missing file: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9939099-8602-4e98-91d4-cb564e3dd9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 36\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Cast the audio column to automatically load audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd89aab3-4c09-4326-824e-26872d9181f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb86bb6e-7d11-4c79-b593-4c18c3531777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2610cd02a64447549ec1efa57f3f38ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Load processor (tokenizer + feature extractor)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     audio = batch[\"audio\"]\n",
    "\n",
    "#     # Extract input features\n",
    "#     batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "\n",
    "#     # Encode labels (transcription)\n",
    "#     batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "\n",
    "#     return batch\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Get input values from audio\n",
    "    input_values = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values[0]\n",
    "\n",
    "    # Get labels from text\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "\n",
    "    # Return proper format for CTC\n",
    "    return {\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": batch[\"labels\"]\n",
    "    }\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, num_proc=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f815e459-77f2-4970-bfa1-dff9350cb6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-large-960h\",\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59761ce1-eda6-4f58-af13-90e2c1a7f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed58c0-4cc1-4663-a45c-2464dbee47f0",
   "metadata": {},
   "source": [
    "# This is a custom attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4336151-5eca-4e83-9d28-c1338f9a2201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Any\n",
    "\n",
    "@dataclass\n",
    "class SimpleCTCDataCollator:\n",
    "        processor: Any\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            # Get the input_values from each feature\n",
    "            input_values = [feature[\"input_values\"].squeeze(0) if isinstance(feature[\"input_values\"], torch.Tensor) else torch.tensor(feature[\"input_values\"]) for feature in features]\n",
    "\n",
    "            # Determine max length for padding\n",
    "            max_length = max(len(x) for x in input_values)\n",
    "\n",
    "            # Pad the input_values manually\n",
    "            padded_input_values = []\n",
    "            attention_mask = []\n",
    "\n",
    "            for val in input_values:\n",
    "                # Create attention mask (1 for real values, 0 for padding)\n",
    "                length = len(val)\n",
    "                mask = torch.ones(length)\n",
    "                if length < max_length:\n",
    "                    pad_length = max_length - length\n",
    "                    # Pad the input values\n",
    "                    val = torch.nn.functional.pad(val, (0, pad_length), value=0.0)\n",
    "                    # Extend the attention mask with zeros for padding\n",
    "                    mask = torch.nn.functional.pad(mask, (0, pad_length), value=0.0)\n",
    "\n",
    "                padded_input_values.append(val)\n",
    "                attention_mask.append(mask)\n",
    "\n",
    "            # Stack the padded inputs and attention masks\n",
    "            batch = {\n",
    "                \"input_values\": torch.stack(padded_input_values),\n",
    "                \"attention_mask\": torch.stack(attention_mask)\n",
    "            }\n",
    "\n",
    "            # Get labels\n",
    "            if \"labels\" in features[0]:\n",
    "                labels = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "                # Pad labels manually with -100 (ignore index for CTC loss)\n",
    "                padded_labels = []\n",
    "                max_label_length = max(len(l) for l in labels)\n",
    "\n",
    "                for label in labels:\n",
    "                    if isinstance(label, torch.Tensor):\n",
    "                        label = label.tolist()\n",
    "\n",
    "                    if len(label) < max_label_length:\n",
    "                        # Pad with -100\n",
    "                        label = label + [-100] * (max_label_length - len(label))\n",
    "\n",
    "                    padded_labels.append(torch.tensor(label, dtype=torch.long))\n",
    "\n",
    "                batch[\"labels\"] = torch.stack(padded_labels)\n",
    "\n",
    "            return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1470fb65-bb60-42e8-b7ac-f98f4ed742f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = SimpleCTCDataCollator(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951c6c2-acf9-4313-8d08-5051688c3030",
   "metadata": {},
   "source": [
    "# This is the version that doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a752fff6-9ae0-4875-b307-299b8c477917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=processor.tokenizer, padding=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee78054-7d3d-4366-ac84-dde99b44e2b6",
   "metadata": {},
   "source": [
    "# Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815a0e11-8ffc-4046-9684-81abdda296d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-creolese-finetuned\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=25,\n",
    "    logging_steps=10,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # True if on GPU with mixed precision\n",
    "    gradient_accumulation_steps=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89cc2afa-4002-4fba-8bb0-36f7a6af925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "import torch\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = torch.argmax(torch.tensor(pred_logits), dim=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = jiwer.wer(label_str, pred_str)\n",
    "    mer = jiwer.mer(label_str, pred_str)\n",
    "    cer = jiwer.cer(label_str, pred_str)\n",
    "    return {\"wer\": wer, \"mer\": mer, \"cer\": cer}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2904d24e-38f8-47e7-850d-c2b2ea023920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing example 1/36\n",
      "Processing example 2/36\n",
      "Processing example 3/36\n",
      "Processing example 4/36\n",
      "Processing example 5/36\n",
      "Processing example 6/36\n",
      "Processing example 7/36\n",
      "Processing example 8/36\n",
      "Processing example 9/36\n",
      "Processing example 10/36\n",
      "Processing example 11/36\n",
      "Processing example 12/36\n",
      "Processing example 13/36\n",
      "Processing example 14/36\n",
      "Processing example 15/36\n",
      "Processing example 16/36\n",
      "Processing example 17/36\n",
      "Processing example 18/36\n",
      "Processing example 19/36\n",
      "Processing example 20/36\n",
      "Processing example 21/36\n",
      "Processing example 22/36\n",
      "Processing example 23/36\n",
      "Processing example 24/36\n",
      "Processing example 25/36\n",
      "Processing example 26/36\n",
      "Processing example 27/36\n",
      "Processing example 28/36\n",
      "Processing example 29/36\n",
      "Processing example 30/36\n",
      "Processing example 31/36\n",
      "Processing example 32/36\n",
      "Processing example 33/36\n",
      "Processing example 34/36\n",
      "Processing example 35/36\n",
      "Processing example 36/36\n"
     ]
    }
   ],
   "source": [
    "# To avoid maxing out ram\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create a directory to store processed features\n",
    "os.makedirs(\"processed_features\", exist_ok=True)\n",
    "\n",
    "# Process each example once and save to disk\n",
    "for idx, example in enumerate(dataset):\n",
    "    print(f\"Processing example {idx+1}/{len(dataset)}\")\n",
    "\n",
    "    # Get audio\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    # Extract features\n",
    "    input_values = processor(\n",
    "        audio[\"array\"], \n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_values[0]\n",
    "\n",
    "    # Get labels\n",
    "    labels = processor.tokenizer(example[\"text\"]).input_ids\n",
    "\n",
    "    # Save to disk\n",
    "    torch.save({\n",
    "        \"input_values\": input_values,\n",
    "        \"labels\": labels\n",
    "    }, f\"processed_features/example_{idx}.pt\")\n",
    "\n",
    "# Create a custom dataset that loads from disk\n",
    "class AudioFeatureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, feature_dir, num_examples):\n",
    "        self.feature_dir = feature_dir\n",
    "        self.num_examples = num_examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load features from disk\n",
    "        features = torch.load(f\"{self.feature_dir}/example_{idx}.pt\")\n",
    "        return features\n",
    "\n",
    "# Use the disk-based dataset\n",
    "train_dataset = AudioFeatureDataset(\"processed_features\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "987bf2f4-e10b-4ee4-805f-4a41498411d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_110193/477740428.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=SimpleCTCDataCollator(processor=processor),\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=processed_dataset,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4016ccd-3b4f-4c57-a03d-622e14cc6cce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b60d5d-1a67-4062-9c29-ffa67a3d6d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n",
    "processor.save_pretrained(\"./wav2vec2-creolese-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af321de-0f77-410a-ba8c-87a69999b867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
