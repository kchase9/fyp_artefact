{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32de85da-7995-424c-b8e7-35d498d5983c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "Successfully installed pip-25.1.1\n",
      "Requirement already satisfied: transformers in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (4.51.3)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: jiwer in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: tensorboard in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (2.19.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: datasets[audio] in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.31.1)\n",
      "Requirement already satisfied: packaging in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (6.0.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.13.1)\n",
      "Requirement already satisfied: librosa in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.11.0)\n",
      "Requirement already satisfied: soxr>=0.4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from datasets[audio]) (0.5.0.post1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (3.11.18)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets[audio]) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets[audio]) (1.1.0)\n",
      "Requirement already satisfied: psutil in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (1.71.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.8)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (80.3.1)\n",
      "Requirement already satisfied: six>1.9 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from tensorboard) (3.1.3)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (4.9.0)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==1.10.0 (from gradio)\n",
      "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (3.0.2)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (11.2.1)\n",
      "Collecting pydantic<2.12,>=2.0 (from gradio)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio)\n",
      "  Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
      "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from gradio) (0.15.3)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.10.0->gradio)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pandas->datasets[audio]) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<2.12,>=2.0->gradio)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets[audio]) (1.20.0)\n",
      "Requirement already satisfied: certifi in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets[audio]) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets[audio]) (2.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.22)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.5.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.1.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from pooch>=1.1->librosa->datasets[audio]) (4.3.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/kris/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages (from scikit-learn>=1.1.0->librosa->datasets[audio]) (3.6.0)\n",
      "Downloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m220.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.10.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m180.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading ruff-0.11.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m214.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, websockets, uvicorn, typing-inspection, tomlkit, semantic-version, ruff, python-multipart, pydantic-core, orjson, groovy, ffmpy, annotated-types, aiofiles, starlette, pydantic, safehttpx, gradio-client, fastapi, gradio, accelerate, evaluate\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [evaluate]\u001b[0m \u001b[32m20/22\u001b[0m [accelerate]io]client]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.6.0 aiofiles-24.1.0 annotated-types-0.7.0 evaluate-0.4.3 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 orjson-3.10.18 pydantic-2.11.4 pydantic-core-2.33.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.8 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 typing-inspection-0.4.0 uvicorn-0.34.2 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2364a5c2-048f-48c9-b87c-b41ec8505eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset, Audio\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c7240-0a97-4be3-9e31-ffbe89e9bf33",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe211bd6-9b93-46f1-8203-edae9bf2311a",
   "metadata": {},
   "source": [
    "Using the guideline: https://huggingface.co/blog/fine-tune-whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8707b470-b0be-48e2-9c32-cd566ab2a03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/2.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/7.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/12.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/13.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/14.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/20.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/21.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/27.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/33.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/34.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/36.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./41.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./47.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./51.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./56.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./57.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./60.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./62.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./63.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./64.wav\n",
      "Found file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./68.wav\n",
      "Missing file: ./creolese-audio-dataset/Audio Files/finetune_eligible/./.wav\n"
     ]
    }
   ],
   "source": [
    "audio_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible\"\n",
    "transcription_path = \"./creolese-audio-dataset/Audio Files/finetune_eligible/transcripts.json\"\n",
    "\n",
    "# Load transcripts JSON\n",
    "with open(transcription_path, 'r') as f:\n",
    "    transcripts = json.load(f)\n",
    "\n",
    "# Create a list of dicts pairing audio files and transcripts\n",
    "data = []\n",
    "for item in transcripts:\n",
    "    audio_file = os.path.join(audio_path, item['audio'])\n",
    "    if os.path.exists(audio_file):\n",
    "        print(f\"Found file: {audio_file}\")\n",
    "        data.append({'audio': audio_file, 'text': item['text']})\n",
    "    else:\n",
    "        print(f\"Missing file: {audio_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c52f6f43-c61a-400b-8a62-006bced68163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'text'],\n",
      "    num_rows: 21\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Cast the audio column to automatically load audio\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8123ecc8-2548-40b5-846a-2659d6165372",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a426221-c1b1-4b98-b757-bbda74bbba1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id = \"openai/whisper-large-v3\"  \n",
    "processor = WhisperProcessor.from_pretrained(model_id, task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29751f87-5195-47e9-a7d3-377262e5c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the language and task for the processor\n",
    "processor.tokenizer.set_prefix_tokens(task=\"transcribe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef341482-1bb9-419c-be42-96bf6a8870f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390e1cba4e9844f39a5487c8fa64ca67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch_prepare_dataset(examples):\n",
    "    audio_arrays = [audio[\"array\"] for audio in examples[\"audio\"]]\n",
    "    sampling_rates = [audio[\"sampling_rate\"] for audio in examples[\"audio\"]]\n",
    "\n",
    "    # Process all examples in a batch\n",
    "    inputs = processor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=sampling_rates[0],  # Assuming all are same rate\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Process all texts in batch\n",
    "    labels = processor.tokenizer(examples[\"text\"], return_tensors=\"pt\", padding=True).input_ids\n",
    "\n",
    "    return {\n",
    "        \"input_features\": inputs[\"input_features\"],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Process in batches\n",
    "prepared_dataset = dataset.map(\n",
    "    batch_prepare_dataset,\n",
    "    batched=True,\n",
    "    batch_size=4,  # Adjust based on memory\n",
    "    remove_columns=dataset.column_names,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "040f7f73-249a-4643-ba3c-4dbe5d591394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.task = \"transcribe\"\n",
    "model.generation_config.language = None \n",
    "model.generation_config.forced_decoder_ids = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a6ac12ad-bd54-41d9-87aa-9138dac8cdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "    \n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "    \n",
    "        # Remove BOS if present\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "    \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor,\n",
    "    decoder_start_token_id=model.config.decoder_start_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22c2edae-d1f1-4d81-89eb-8b18da502c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer\n",
    "\n",
    "transform = jiwer.Compose([\n",
    "        jiwer.ToLowerCase(),\n",
    "        jiwer.RemovePunctuation(),\n",
    "        jiwer.Strip(),\n",
    "        jiwer.RemoveMultipleSpaces(),\n",
    "])\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = jiwer.wer(\n",
    "        ground_truth=label_str,\n",
    "        hypothesis=pred_str,\n",
    "        truth_transform=transform,\n",
    "        hypothesis_transform=transform\n",
    "    ) \n",
    "    mer = jiwer.mer(\n",
    "        ground_truth=label_str,\n",
    "        hypothesis=pred_str,\n",
    "        truth_transform=transform,\n",
    "        hypothesis_transform=transform\n",
    "    ) \n",
    "    cer = jiwer.cer(\n",
    "        ground_truth=label_str,\n",
    "        hypothesis=pred_str,\n",
    "        truth_transform=transform,\n",
    "        hypothesis_transform=transform\n",
    "    ) \n",
    "    return {\"wer\": wer, \"cer\": cer, \"mer\": mer}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13fb7c94-cd4f-469d-b3be-9ce3d4f79f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-large-v3-creolese-finetuned\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,  \n",
    "    gradient_checkpointing=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    do_eval= True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"tensorboard\"],  # or [\"none\"]\n",
    "    push_to_hub=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d5b7ba82-333d-41cf-b5be-cfd3c124316c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109158/2690013130.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=prepared_dataset,\n",
    "    eval_dataset=None,  # or add eval split if available\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f15a5426-2a28-45a9-a015-71d61cd680b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:192\u001b[39m, in \u001b[36mBatchFeature.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     tensor = \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m[key] = tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:141\u001b[39m, in \u001b[36mBatchFeature._get_is_as_tensor_fns.<locals>.as_tensor\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[32m0\u001b[39m], np.ndarray):\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     value = np.array(value)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    143\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(value[\u001b[32m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m))\n\u001b[32m    144\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value[\u001b[32m0\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m], np.ndarray)\n\u001b[32m    146\u001b[39m ):\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (8, 128) + inhomogeneous part.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:2514\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2512\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2513\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2514\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2515\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2516\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/trainer.py:5243\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5241\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5243\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5244\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5245\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/accelerate/data_loader.py:566\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mDataCollatorSpeechSeq2SeqWithPadding.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Union[List[\u001b[38;5;28mint\u001b[39m], torch.Tensor]]]) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m     11\u001b[39m     input_features = [{\u001b[33m\"\u001b[39m\u001b[33minput_features\u001b[39m\u001b[33m\"\u001b[39m: f[\u001b[33m\"\u001b[39m\u001b[33minput_features\u001b[39m\u001b[33m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     label_features = [{\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: f[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[32m     15\u001b[39m     labels_batch = \u001b[38;5;28mself\u001b[39m.processor.tokenizer.pad(label_features, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/feature_extraction_sequence_utils.py:224\u001b[39m, in \u001b[36mSequenceFeatureExtractor.pad\u001b[39m\u001b[34m(self, processed_features, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors)\u001b[39m\n\u001b[32m    221\u001b[39m             value = value.astype(np.float32)\n\u001b[32m    222\u001b[39m         batch_outputs[key].append(value)\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchFeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:78\u001b[39m, in \u001b[36mBatchFeature.__init__\u001b[39m\u001b[34m(self, data, tensor_type)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m, tensor_type: Union[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m, TensorType] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     77\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(data)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fyp/fyp_artefact/fyp_env/lib/python3.11/site-packages/transformers/feature_extraction_utils.py:198\u001b[39m, in \u001b[36mBatchFeature.convert_to_tensors\u001b[39m\u001b[34m(self, tensor_type)\u001b[39m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33moverflowing_values\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    197\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor returning overflowing values of different lengths. \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    199\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnable to create tensor, you should probably activate padding \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwith \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpadding=True\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to have batched tensors with the same length.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m         )\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length."
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
